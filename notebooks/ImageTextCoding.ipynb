{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac5f32466488470eb0b98da553e6bc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89cc39062a414a6497a254c88e1af082",
              "IPY_MODEL_a062a6f4eeb94ccbb83bf8bae21b30ba",
              "IPY_MODEL_c20c0b658f9243d18ca7e4e9a2658e1f"
            ],
            "layout": "IPY_MODEL_61627c7fa40b46758d83675e9a3a0065"
          }
        },
        "89cc39062a414a6497a254c88e1af082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907610f2b6f546ab9ce91c57529defc6",
            "placeholder": "​",
            "style": "IPY_MODEL_46b760dc7c814dee8004ee59b585d4fa",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "a062a6f4eeb94ccbb83bf8bae21b30ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0de6897ea441198acd9801353c0ca4",
            "max": 3944692325,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_927142ced8e946c2ab2e1be0989cee79",
            "value": 3944692325
          }
        },
        "c20c0b658f9243d18ca7e4e9a2658e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d32a0d9363124aa4af70ac58d5d3cb81",
            "placeholder": "​",
            "style": "IPY_MODEL_ac6740afa7db4008b56c9bfccbf2f419",
            "value": " 3.94G/3.94G [00:10&lt;00:00, 431MB/s]"
          }
        },
        "61627c7fa40b46758d83675e9a3a0065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907610f2b6f546ab9ce91c57529defc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b760dc7c814dee8004ee59b585d4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d0de6897ea441198acd9801353c0ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927142ced8e946c2ab2e1be0989cee79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d32a0d9363124aa4af70ac58d5d3cb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6740afa7db4008b56c9bfccbf2f419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L00NE/loone_tmp_scripts/blob/main/notebooks/ImageTextCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Description:\n",
        " This is demo inference code of the paper **LMM-driven Semantic Text-Image Coding for Ultra Low-bitrate Learned Image Compression**\n",
        "\n",
        " Main codes are from MISC repository (https://github.com/lcysyzxdxc/MISC ) but I modified some code so that it works on newer version of DiffBIR.\n",
        "\n",
        " # Usage:\n",
        " To run DiffBIR, colab PRO is required. Set L4 GPU & 64GB RAM in runtime setting to run this code."
      ],
      "metadata": {
        "id": "ceO9_JR37tv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "Ee26wmt53ux2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fC_zWGvH-G8a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# %cd /content\n",
        "# !git clone https://github.com/user475289/ImageTextCoding\n",
        "# %cd /content/ImageTextCoding\n",
        "# !mkdir weights\n",
        "# !pip install -q einops pytorch_lightning==1.9.5 torch==2.1.0 gradio omegaconf transformers lpips segment_anything #opencv-python\n",
        "# !pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "# !pip install -q git+https://github.com/mlfoundations/open_clip@v2.20.0\n",
        "\n",
        "# !pip install compressai\n",
        "# !apt -y install -qq aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lxq007/DiffBIR/resolve/main/general_full_v1.ckpt -d /content/ImageTextCoding/weights -o general_full_v1.ckpt\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt -d /content/ImageTextCoding/weights -o general_swinir_v1.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import io\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageChops\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "pInmY6iL-g_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "from clip import clip\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from  matplotlib import pyplot as plt\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from torchvision.transforms import InterpolationMode\n",
        "BICUBIC = InterpolationMode.BICUBIC\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "preprocess =  Compose([Resize((224, 224), interpolation=BICUBIC), ToTensor(),\n",
        "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n",
        "seg_model, preprocess = clip.load(\"CS-ViT-B/16\", device=device)\n",
        "#NOTE: This model is in CLIP-surgery. Not included in clip-openai.\n",
        "seg_model.eval()"
      ],
      "metadata": {
        "id": "Oqz3zyAbAtaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "PgGJGkqy4ER0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.transform import resize\n",
        "from utils.image import wavelet_reconstruction\n",
        "\n",
        "import shutil\n",
        "def clear_directory(directory_path):\n",
        "# 再帰的にディレクトリを消去する\n",
        "# Recurrently delete specified directory\n",
        "    for file in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, file)\n",
        "        if os.path.isdir(file_path):\n",
        "\n",
        "            for root, dirs, files in os.walk(file_path):\n",
        "                for file in files:\n",
        "                    os.remove(os.path.join(root, file))\n",
        "\n",
        "\n",
        "\n",
        "def to_block(img,grid=32,level=8):\n",
        "# 画像を決定的にダウンサンプルする\n",
        "    g_w=int(img.size[0]/grid)\n",
        "    g_h=int(img.size[1]/grid)# 縦横とも 1/grid 倍する\n",
        "    img_resize=img.resize((g_w, g_h))\n",
        "    img_np=np.floor(np.array(img_resize)/level)*level\n",
        "    img_np=img_np.astype (np.uint8)\n",
        "    img_reference = Image.fromarray(img_np).resize(img.size)\n",
        "    return img_reference, g_w, g_h\n",
        "\n",
        "\n",
        "def divide_integer(num, n):\n",
        "    quotient = num // n  # 整数除法，计算商\n",
        "    remainder = num % n  # 取余数\n",
        "    result = [quotient] * n  # 创建一个包含n个quotient的列表\n",
        "\n",
        "    # 将余数平均分配给前几个数\n",
        "    for i in range(remainder):\n",
        "        result[i] += 1\n",
        "\n",
        "    return result\n",
        "def mask_block(mask,num=8,level=0.35):\n",
        "    tmp=resize(mask, (num, num), mode='reflect')\n",
        "    tmp[tmp>level]=255\n",
        "    tmp[tmp<=level]=0\n",
        "    rp_mat_0=np.array(divide_integer(mask.shape[0], num),dtype='int')\n",
        "    rp_mat_1=np.array(divide_integer(mask.shape[1], num),dtype='int')\n",
        "    return tmp.repeat(rp_mat_1,axis=1).repeat(rp_mat_0,axis=0)\n",
        "\n",
        "def image_paddle_in(image, num=32):\n",
        "    # 计算扩充后的宽度和高度\n",
        "    new_width = ((image.width-1) // num + 1) * num\n",
        "    new_height = ((image.height-1) // num + 1) * num\n",
        "\n",
        "    # 创建一个新的扩充后的图像，用空值填充\n",
        "    new_image = Image.new(\"RGB\", (new_width, new_height), (0, 0, 0))\n",
        "\n",
        "    # 将原始图像粘贴到扩充后的图像左上角\n",
        "    new_image.paste(image, (0, 0))\n",
        "    return new_image,image.width,image.height\n",
        "\n",
        "def image_paddle_out(image, old_width, old_height):\n",
        "\n",
        "    return image.crop((0,0,old_width,old_height))"
      ],
      "metadata": {
        "id": "QGT95A8pz-ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_bpp(out_net):\n",
        "    size = out_net['x_hat'].size()\n",
        "    num_pixels = size[0] * size[2] * size[3]\n",
        "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
        "              for likelihoods in out_net['likelihoods'].values()).item()\n",
        "\n",
        "def clip_map(img,texts,mask_num=8):\n",
        "    image = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CLIP architecture surgery acts on the image encoder\n",
        "        cv2_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "        image_features = seg_model.encode_image(image)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # Prompt ensemble for text features with normalization\n",
        "        text_features = clip.encode_text_with_prompt_ensemble(seg_model, texts, device)\n",
        "\n",
        "        # Extract redundant features from an empty string\n",
        "        redundant_features = clip.encode_text_with_prompt_ensemble(seg_model, [\"\"], device)\n",
        "\n",
        "        # Apply feature surgery for single text\n",
        "        similarity = clip.clip_feature_surgery(image_features, text_features, redundant_features)\n",
        "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], cv2_img.shape[:2])\n",
        "\n",
        "        mask_0=(similarity_map[0,:,:,0].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_1=(similarity_map[0,:,:,1].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_2=(similarity_map[0,:,:,2].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_0=Image.fromarray(mask_block(mask_0,num=mask_num))\n",
        "        mask_1=Image.fromarray(mask_block(mask_1,num=mask_num))\n",
        "        mask_2=Image.fromarray(mask_block(mask_2,num=mask_num))\n",
        "        return mask_0,mask_1,mask_2\n",
        "\n",
        "def sr_pipe(img_reference,positive_prompt=\"\",cfg=1.0,steps=40,res=512, cond_scale = 1.0, old_size = None):\n",
        "    control_img = img_reference\n",
        "    sr_scale = 1\n",
        "    num_samples = 1\n",
        "    #image_size = old_size\n",
        "    disable_preprocess_model= False\n",
        "    strength = 1.0\n",
        "    cond_scale = 1.0\n",
        "    use_color_fix = True\n",
        "    keep_original_size = False\n",
        "    negative_prompt=\"Blurry, Low Quality\"\n",
        "    sampler = SpacedSampler(model, var_type=\"fixed_small\")\n",
        "\n",
        "    if sr_scale != 1:\n",
        "        control_img = control_img.resize(\n",
        "            tuple(math.ceil(x * sr_scale) for x in control_img.size),\n",
        "            Image.BICUBIC\n",
        "        )\n",
        "    input_size = control_img.size\n",
        "    #control_img = auto_resize(control_img, image_size)\n",
        "    h, w = control_img.height, control_img.width\n",
        "    control_img = pad(np.array(control_img), scale=64) # HWC, RGB, [0, 255]\n",
        "    control_imgs = [control_img] * num_samples\n",
        "    control = torch.tensor(np.stack(control_imgs) / 255.0, dtype=torch.float32, device=model.device).clamp_(0, 1)\n",
        "    control = einops.rearrange(control, \"n h w c -> n c h w\").contiguous()\n",
        "    if not disable_preprocess_model:\n",
        "        control = model.preprocess_model(control)\n",
        "    height, width = control.size(-2), control.size(-1)\n",
        "    cond = {\n",
        "        \"c_latent\": [model.apply_condition_encoder(control)],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([positive_prompt] * num_samples)]\n",
        "    }\n",
        "    uncond = {\n",
        "        \"c_latent\": [model.apply_condition_encoder(control)],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([negative_prompt] * num_samples)]\n",
        "    }\n",
        "    model.control_scales = [strength] * 13\n",
        "\n",
        "    shape = (num_samples, 4, height // 8, width // 8)\n",
        "    print(f\"latent shape = {shape}\")\n",
        "    x_T = torch.randn(shape, device=model.device, dtype=torch.float32)\n",
        "    samples = sampler.sample(\n",
        "        steps, shape, cond,\n",
        "        unconditional_guidance_scale=cond_scale,\n",
        "        unconditional_conditioning=uncond,\n",
        "        cond_fn=None, x_T=x_T\n",
        "    )\n",
        "    x_samples = model.decode_first_stage(samples)\n",
        "    x_samples = ((x_samples + 1) / 2).clamp(0, 1)\n",
        "\n",
        "    # apply color correction\n",
        "    if use_color_fix:\n",
        "        x_samples = wavelet_reconstruction(x_samples, control)\n",
        "\n",
        "    x_samples = (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 255).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
        "    preds = []\n",
        "    for img in x_samples:\n",
        "        if keep_original_size:\n",
        "            # remove padding and resize to input size\n",
        "            img = Image.fromarray(img[:h, :w, :]).resize(input_size, Image.LANCZOS)\n",
        "            preds.append(np.array(img))\n",
        "        else:\n",
        "            # remove padding\n",
        "            preds.append(img[:h, :w, :])\n",
        "    return preds"
      ],
      "metadata": {
        "id": "fPORy80RBMXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/user475289/ImageTextCoding/releases/download/v0.1-alpha/10ep_cheng_3_mse0.5_vgg0.2_i2t0.2_iqa0.1.tar"
      ],
      "metadata": {
        "id": "1WnwPgCkD5Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode='tuned_net'\n",
        "using_map=False\n",
        "\n",
        "if mode=='net':\n",
        "    from compressai.zoo import cheng2020_attn\n",
        "    comp_net = cheng2020_attn(pretrained=True, quality = 1).to(device)\n",
        "elif mode=='tuned_net':\n",
        "    checkpoint_path= '/content/ImageTextCoding/10ep_cheng_3_mse0.5_vgg0.2_i2t0.2_iqa0.1.tar'\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    state_dict = checkpoint\n",
        "    for key in [\"network\", \"state_dict\", \"model_state_dict\"]:\n",
        "        if key in checkpoint:\n",
        "            state_dict = checkpoint[key]\n",
        "    arch='cheng2020-attn'\n",
        "    from compressai.zoo.image import model_architectures as architectures\n",
        "    model_cls = architectures[arch]\n",
        "    comp_net = model_cls.from_state_dict(state_dict).eval().to(device)\n",
        "elif mode=='ref':\n",
        "    ref_path='./ref/example-reference.png'\n",
        "    ref_bpp=0.0421\n",
        "elif mode=='pixel':\n",
        "    block_level=3\n",
        "    block_num_min=32\n",
        "\n",
        "\n",
        "mask_num=8\n",
        "res=1024\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/ImageTextCoding/kodak_llava_1.5.csv\")\n",
        "\n",
        "image_path='/content/ImageTextCoding/kodim15.png'\n",
        "\n",
        "\n",
        "img = Image.open(image_path).convert('RGB')"
      ],
      "metadata": {
        "id": "zwkrbu2DC0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Encoder ##########################\n",
        "\n",
        "#GPT prompt processing\n",
        "'''\n",
        "prompt=df['prompt'][0]\n",
        "prompt_list=prompt.split('\\n')\n",
        "prompt_list = [element for element in prompt_list if element != '']\n",
        "name_0,name_1,name_2=prompt_list[0].split('.')[0].split(',')\n",
        "detail_0,detail_1,detail_2=prompt_list[1],prompt_list[2],prompt_list[3]\n",
        "detail_all=prompt_list[4]\n",
        "'''\n",
        "name_0 = df['item1'][0]\n",
        "name_1 = df['item2'][0]\n",
        "name_2 = df['item3'][0]\n",
        "detail_0 = df['item1_description'][0]\n",
        "detail_1 = df['item2_description'][0]\n",
        "detail_2 = df['item3_description'][0]\n",
        "detail_all = df['overall_description'][0]\n",
        "\n",
        "mask_0,mask_1,mask_2=clip_map(img,[name_0,name_1,name_2],mask_num)\n",
        "\n",
        "\n",
        "#reference\n",
        "if mode=='pixel':\n",
        "    old_width, old_height=img.size\n",
        "    block_num=max(int(max(old_width, old_height)/16),block_num_min)\n",
        "    img_reference=to_block(img,block_num,2**block_level)\n",
        "\n",
        "    b_image=block_level*block_num**2\n",
        "elif mode=='net' or mode =='tuned_net':\n",
        "    img, old_width, old_height = image_paddle_in(img, 64)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out_net = comp_net.forward(x)\n",
        "    out_net['x_hat'].clamp_(0, 1)\n",
        "    img_reference = transforms.ToPILImage()(out_net['x_hat'].squeeze().cpu())\n",
        "    img_reference = image_paddle_out(img_reference, old_width, old_height)\n",
        "    b_image=compute_bpp(out_net)*img.size[0]*img.size[1]\n",
        "elif mode=='ref':\n",
        "    old_width, old_height=img.size\n",
        "    img_reference = Image.open(ref_path).convert('RGB')\n",
        "    b_image=ref_bpp*img.size[0]*img.size[1]\n",
        "\n",
        "\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "GRUiSYokEoT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.image import auto_resize, pad\n",
        "import einops\n",
        "from utils.common import instantiate_from_config, load_state_dict\n",
        "from omegaconf import OmegaConf\n",
        "from model.cldm import ControlLDM\n",
        "from model.spaced_sampler import SpacedSampler\n",
        "model: ControlLDM = instantiate_from_config(OmegaConf.load('./configs/model/cldm.yaml'))\n",
        "ckpt_swinir='./weights/general_full_v1.ckpt'\n",
        "#ckpt_net='./weights/cheng_small.pth.tar'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ac5f32466488470eb0b98da553e6bc93",
            "89cc39062a414a6497a254c88e1af082",
            "a062a6f4eeb94ccbb83bf8bae21b30ba",
            "c20c0b658f9243d18ca7e4e9a2658e1f",
            "61627c7fa40b46758d83675e9a3a0065",
            "907610f2b6f546ab9ce91c57529defc6",
            "46b760dc7c814dee8004ee59b585d4fa",
            "4d0de6897ea441198acd9801353c0ca4",
            "927142ced8e946c2ab2e1be0989cee79",
            "d32a0d9363124aa4af70ac58d5d3cb81",
            "ac6740afa7db4008b56c9bfccbf2f419"
          ]
        },
        "id": "Bf7ofKGFK9ZS",
        "outputId": "e0dad7f6-d822-430a-b023-a25dd2c6a1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
            "  rank_zero_deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ControlLDM: Running in eps-prediction mode\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "DiffusionWrapper has 865.91 M params.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5f32466488470eb0b98da553e6bc93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 187MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_state_dict(model, torch.load(ckpt_swinir, map_location=\"cpu\"), strict=True)\n",
        "model.freeze()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "05800gYuPqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Decoder ##########################\n",
        "num_inference_steps=40\n",
        "exag=1024/max(img_reference.size)\n",
        "height=int(img_reference.size[1]*exag/8)*8\n",
        "width=int(img_reference.size[0]*exag/8)*8\n",
        "\n",
        "#    img_reference=img_reference.resize([width,height])\n",
        "mask_0=mask_0.resize([width,height])\n",
        "mask_1=mask_1.resize([width,height])\n",
        "mask_2=mask_2.resize([width,height])\n",
        "mask_all=Image.new(\"RGB\", img_reference.size, (255, 255, 255))\n",
        "\n",
        "image = img_reference\n",
        "\n",
        "if using_map:\n",
        "    b_mask=mask_num*mask_num*3\n",
        "    b_word=(len(detail_0)+len(detail_1)+len(detail_2)+len(detail_all))*8\n",
        "    bpp=(b_image+b_mask+b_word)/(img.size[0]*img.size[1])\n",
        "    print('bpp='+str(bpp))\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_0,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_0.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_0)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask0/'+image_name)\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_1,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_1.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_1)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask1/'+image_name)\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_2,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_2.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_2)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask2/'+image_name)\n",
        "\n",
        "    image = sr_pipe(image,positive_prompt=detail_all,cfg=7,steps=40,res=res).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'SR/'+image_name)\n",
        "\n",
        "else:\n",
        "    b_word=(len(detail_all))*8\n",
        "    bpp=(b_image+b_word)/(img.size[0]*img.size[1])\n",
        "    print('image bit = ', b_image, 'text bit = ', b_word, 'bpp=', str(bpp))\n",
        "\n",
        "    image = sr_pipe(image,positive_prompt=detail_all,cfg=7,steps=40,res=res,\n",
        "                    old_size = (old_width, old_height))\n",
        "    #output_image = image.resize((old_width, old_height))"
      ],
      "metadata": {
        "id": "cCVF3b3eI2lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d7b7f5-cbf9-487e-c673-a596d742ef8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image bit =  12702.57421875 text bit =  2888 bpp= 0.03964888056119283\n",
            "latent shape = (1, 4, 64, 96)\n",
            "start to sample from a given noise\n",
            "Running Spaced Sampling with 40 timesteps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spaced Sampler: 100%|██████████| 40/40 [00:08<00:00,  4.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Image (Compressed & Decompressed & Deblurred)"
      ],
      "metadata": {
        "id": "uNEusZU0eOfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image[0]"
      ],
      "metadata": {
        "id": "GDS0siKav1CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Image"
      ],
      "metadata": {
        "id": "MsyQGpCLefx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "94ZpiVgvWNyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "from lpips import LPIPS\n",
        "\n",
        "\n",
        "def psnr(img0, img1):\n",
        "    mse = np.mean((img0 - img1) ** 2)\n",
        "    return 10 * np.log10(255 ** 2 / mse)\n",
        "\n",
        "def lpips(img0, img1):\n",
        "    # Variables im0, im1 is a PyTorch Tensor/Variable with shape Nx3xHxW\n",
        "    # (N patches of size HxW, RGB images scaled in [-1,+1])\n",
        "    loss_fn_vgg = LPIPS(net='vgg')\n",
        "    img0 = (TF.to_tensor(img0) - 0.5) * 2\n",
        "    img0.unsqueeze(0)\n",
        "\n",
        "    img1 = (TF.to_tensor(img1) - 0.5) * 2\n",
        "    img1.unsqueeze(0)\n",
        "    # Higher means further/more different. Lower means more similar.\n",
        "    return loss_fn_vgg(img0, img1).item()\n",
        "\n"
      ],
      "metadata": {
        "id": "VByb43NHxnFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.array(img, dtype = np.float32)\n",
        "image[0] = np.array(image[0], dtype = np.float32)"
      ],
      "metadata": {
        "id": "kca2QZKG9vmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpips: \", lpips(img, image[0]))\n",
        "print(\"psnr: \", psnr(img, image[0]))"
      ],
      "metadata": {
        "id": "nQiJOLyhUQKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhfY-AOc_qB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}