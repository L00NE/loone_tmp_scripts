{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L00NE/loone_tmp_scripts/blob/main/notebooks/ImageTextCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Description:\n",
        " This is demo inference code of the paper **LMM-driven Semantic Text-Image Coding for Ultra Low-bitrate Learned Image Compression**\n",
        "\n",
        " Main codes are from MISC repository (https://github.com/lcysyzxdxc/MISC ) but I modified some code so that it works on newer version of DiffBIR.\n",
        "\n",
        " # Usage:\n",
        " To run DiffBIR, colab PRO is required. Set L4 GPU & 64GB RAM in runtime setting to run this code."
      ],
      "metadata": {
        "id": "ceO9_JR37tv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "Ee26wmt53ux2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fC_zWGvH-G8a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# %cd /content\n",
        "# !git clone https://github.com/user475289/ImageTextCoding\n",
        "# %cd /content/ImageTextCoding\n",
        "# !mkdir weights\n",
        "# !pip install -q einops pytorch_lightning==1.9.5 torch==2.1.0 gradio omegaconf transformers lpips segment_anything #opencv-python\n",
        "# !pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "# !pip install -q git+https://github.com/mlfoundations/open_clip@v2.20.0\n",
        "\n",
        "# !pip install compressai\n",
        "# !apt -y install -qq aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lxq007/DiffBIR/resolve/main/general_full_v1.ckpt -d /content/ImageTextCoding/weights -o general_full_v1.ckpt\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt -d /content/ImageTextCoding/weights -o general_swinir_v1.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import io\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageChops\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "pInmY6iL-g_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "from clip import clip\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from  matplotlib import pyplot as plt\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from torchvision.transforms import InterpolationMode\n",
        "BICUBIC = InterpolationMode.BICUBIC\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "preprocess =  Compose([Resize((224, 224), interpolation=BICUBIC), ToTensor(),\n",
        "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n",
        "seg_model, preprocess = clip.load(\"CS-ViT-B/16\", device=device)\n",
        "#NOTE: This model is in CLIP-surgery. Not included in clip-openai.\n",
        "seg_model.eval()"
      ],
      "metadata": {
        "id": "Oqz3zyAbAtaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "PgGJGkqy4ER0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.transform import resize\n",
        "from utils.image import wavelet_reconstruction\n",
        "\n",
        "import shutil\n",
        "def clear_directory(directory_path):\n",
        "# 再帰的にディレクトリを消去する\n",
        "# Recurrently delete specified directory\n",
        "    for file in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, file)\n",
        "        if os.path.isdir(file_path):\n",
        "\n",
        "            for root, dirs, files in os.walk(file_path):\n",
        "                for file in files:\n",
        "                    os.remove(os.path.join(root, file))\n",
        "\n",
        "\n",
        "\n",
        "def to_block(img,grid=32,level=8):\n",
        "# 画像を決定的にダウンサンプルする\n",
        "    g_w=int(img.size[0]/grid)\n",
        "    g_h=int(img.size[1]/grid)# 縦横とも 1/grid 倍する\n",
        "    img_resize=img.resize((g_w, g_h))\n",
        "    img_np=np.floor(np.array(img_resize)/level)*level\n",
        "    img_np=img_np.astype (np.uint8)\n",
        "    img_reference = Image.fromarray(img_np).resize(img.size)\n",
        "    return img_reference, g_w, g_h\n",
        "\n",
        "\n",
        "def divide_integer(num, n):\n",
        "    quotient = num // n  # 整数除法，计算商\n",
        "    remainder = num % n  # 取余数\n",
        "    result = [quotient] * n  # 创建一个包含n个quotient的列表\n",
        "\n",
        "    # 将余数平均分配给前几个数\n",
        "    for i in range(remainder):\n",
        "        result[i] += 1\n",
        "\n",
        "    return result\n",
        "def mask_block(mask,num=8,level=0.35):\n",
        "    tmp=resize(mask, (num, num), mode='reflect')\n",
        "    tmp[tmp>level]=255\n",
        "    tmp[tmp<=level]=0\n",
        "    rp_mat_0=np.array(divide_integer(mask.shape[0], num),dtype='int')\n",
        "    rp_mat_1=np.array(divide_integer(mask.shape[1], num),dtype='int')\n",
        "    return tmp.repeat(rp_mat_1,axis=1).repeat(rp_mat_0,axis=0)\n",
        "\n",
        "def image_paddle_in(image, num=32):\n",
        "    # 计算扩充后的宽度和高度\n",
        "    new_width = ((image.width-1) // num + 1) * num\n",
        "    new_height = ((image.height-1) // num + 1) * num\n",
        "\n",
        "    # 创建一个新的扩充后的图像，用空值填充\n",
        "    new_image = Image.new(\"RGB\", (new_width, new_height), (0, 0, 0))\n",
        "\n",
        "    # 将原始图像粘贴到扩充后的图像左上角\n",
        "    new_image.paste(image, (0, 0))\n",
        "    return new_image,image.width,image.height\n",
        "\n",
        "def image_paddle_out(image, old_width, old_height):\n",
        "\n",
        "    return image.crop((0,0,old_width,old_height))"
      ],
      "metadata": {
        "id": "QGT95A8pz-ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_bpp(out_net):\n",
        "    size = out_net['x_hat'].size()\n",
        "    num_pixels = size[0] * size[2] * size[3]\n",
        "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
        "              for likelihoods in out_net['likelihoods'].values()).item()\n",
        "\n",
        "def clip_map(img,texts,mask_num=8):\n",
        "    image = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CLIP architecture surgery acts on the image encoder\n",
        "        cv2_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "        image_features = seg_model.encode_image(image)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # Prompt ensemble for text features with normalization\n",
        "        text_features = clip.encode_text_with_prompt_ensemble(seg_model, texts, device)\n",
        "\n",
        "        # Extract redundant features from an empty string\n",
        "        redundant_features = clip.encode_text_with_prompt_ensemble(seg_model, [\"\"], device)\n",
        "\n",
        "        # Apply feature surgery for single text\n",
        "        similarity = clip.clip_feature_surgery(image_features, text_features, redundant_features)\n",
        "        similarity_map = clip.get_similarity_map(similarity[:, 1:, :], cv2_img.shape[:2])\n",
        "\n",
        "        mask_0=(similarity_map[0,:,:,0].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_1=(similarity_map[0,:,:,1].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_2=(similarity_map[0,:,:,2].cpu().numpy() * 255).astype('uint8')\n",
        "        mask_0=Image.fromarray(mask_block(mask_0,num=mask_num))\n",
        "        mask_1=Image.fromarray(mask_block(mask_1,num=mask_num))\n",
        "        mask_2=Image.fromarray(mask_block(mask_2,num=mask_num))\n",
        "        return mask_0,mask_1,mask_2\n",
        "\n",
        "def sr_pipe(img_reference,positive_prompt=\"\",cfg=1.0,steps=40,res=512, cond_scale = 1.0, old_size = None):\n",
        "    control_img = img_reference\n",
        "    sr_scale = 1\n",
        "    num_samples = 1\n",
        "    #image_size = old_size\n",
        "    disable_preprocess_model= False\n",
        "    strength = 1.0\n",
        "    cond_scale = 1.0\n",
        "    use_color_fix = True\n",
        "    keep_original_size = False\n",
        "    negative_prompt=\"Blurry, Low Quality\"\n",
        "    sampler = SpacedSampler(model, var_type=\"fixed_small\")\n",
        "\n",
        "    if sr_scale != 1:\n",
        "        control_img = control_img.resize(\n",
        "            tuple(math.ceil(x * sr_scale) for x in control_img.size),\n",
        "            Image.BICUBIC\n",
        "        )\n",
        "    input_size = control_img.size\n",
        "    #control_img = auto_resize(control_img, image_size)\n",
        "    h, w = control_img.height, control_img.width\n",
        "    control_img = pad(np.array(control_img), scale=64) # HWC, RGB, [0, 255]\n",
        "    control_imgs = [control_img] * num_samples\n",
        "    control = torch.tensor(np.stack(control_imgs) / 255.0, dtype=torch.float32, device=model.device).clamp_(0, 1)\n",
        "    control = einops.rearrange(control, \"n h w c -> n c h w\").contiguous()\n",
        "    if not disable_preprocess_model:\n",
        "        control = model.preprocess_model(control)\n",
        "    height, width = control.size(-2), control.size(-1)\n",
        "    cond = {\n",
        "        \"c_latent\": [model.apply_condition_encoder(control)],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([positive_prompt] * num_samples)]\n",
        "    }\n",
        "    uncond = {\n",
        "        \"c_latent\": [model.apply_condition_encoder(control)],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([negative_prompt] * num_samples)]\n",
        "    }\n",
        "    model.control_scales = [strength] * 13\n",
        "\n",
        "    shape = (num_samples, 4, height // 8, width // 8)\n",
        "    print(f\"latent shape = {shape}\")\n",
        "    x_T = torch.randn(shape, device=model.device, dtype=torch.float32)\n",
        "    samples = sampler.sample(\n",
        "        steps, shape, cond,\n",
        "        unconditional_guidance_scale=cond_scale,\n",
        "        unconditional_conditioning=uncond,\n",
        "        cond_fn=None, x_T=x_T\n",
        "    )\n",
        "    x_samples = model.decode_first_stage(samples)\n",
        "    x_samples = ((x_samples + 1) / 2).clamp(0, 1)\n",
        "\n",
        "    # apply color correction\n",
        "    if use_color_fix:\n",
        "        x_samples = wavelet_reconstruction(x_samples, control)\n",
        "\n",
        "    x_samples = (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 255).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
        "    preds = []\n",
        "    for img in x_samples:\n",
        "        if keep_original_size:\n",
        "            # remove padding and resize to input size\n",
        "            img = Image.fromarray(img[:h, :w, :]).resize(input_size, Image.LANCZOS)\n",
        "            preds.append(np.array(img))\n",
        "        else:\n",
        "            # remove padding\n",
        "            preds.append(img[:h, :w, :])\n",
        "    return preds"
      ],
      "metadata": {
        "id": "fPORy80RBMXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/user475289/ImageTextCoding/releases/download/v0.1-alpha/10ep_cheng_3_mse0.5_vgg0.2_i2t0.2_iqa0.1.tar"
      ],
      "metadata": {
        "id": "1WnwPgCkD5Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode='tuned_net'\n",
        "using_map=False\n",
        "\n",
        "if mode=='net':\n",
        "    from compressai.zoo import cheng2020_attn\n",
        "    comp_net = cheng2020_attn(pretrained=True, quality = 1).to(device)\n",
        "elif mode=='tuned_net':\n",
        "    checkpoint_path= '/content/ImageTextCoding/10ep_cheng_3_mse0.5_vgg0.2_i2t0.2_iqa0.1.tar'\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    state_dict = checkpoint\n",
        "    for key in [\"network\", \"state_dict\", \"model_state_dict\"]:\n",
        "        if key in checkpoint:\n",
        "            state_dict = checkpoint[key]\n",
        "    arch='cheng2020-attn'\n",
        "    from compressai.zoo.image import model_architectures as architectures\n",
        "    model_cls = architectures[arch]\n",
        "    comp_net = model_cls.from_state_dict(state_dict).eval().to(device)\n",
        "elif mode=='ref':\n",
        "    ref_path='./ref/example-reference.png'\n",
        "    ref_bpp=0.0421\n",
        "elif mode=='pixel':\n",
        "    block_level=3\n",
        "    block_num_min=32\n",
        "\n",
        "\n",
        "mask_num=8\n",
        "res=1024\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/ImageTextCoding/kodak_llava_1.5.csv\")\n",
        "\n",
        "image_path='/content/ImageTextCoding/kodim15.png'\n",
        "\n",
        "\n",
        "img = Image.open(image_path).convert('RGB')"
      ],
      "metadata": {
        "id": "zwkrbu2DC0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Encoder ##########################\n",
        "\n",
        "#GPT prompt processing\n",
        "'''\n",
        "prompt=df['prompt'][0]\n",
        "prompt_list=prompt.split('\\n')\n",
        "prompt_list = [element for element in prompt_list if element != '']\n",
        "name_0,name_1,name_2=prompt_list[0].split('.')[0].split(',')\n",
        "detail_0,detail_1,detail_2=prompt_list[1],prompt_list[2],prompt_list[3]\n",
        "detail_all=prompt_list[4]\n",
        "'''\n",
        "name_0 = df['item1'][0]\n",
        "name_1 = df['item2'][0]\n",
        "name_2 = df['item3'][0]\n",
        "detail_0 = df['item1_description'][0]\n",
        "detail_1 = df['item2_description'][0]\n",
        "detail_2 = df['item3_description'][0]\n",
        "detail_all = df['overall_description'][0]\n",
        "\n",
        "mask_0,mask_1,mask_2=clip_map(img,[name_0,name_1,name_2],mask_num)\n",
        "\n",
        "\n",
        "#reference\n",
        "if mode=='pixel':\n",
        "    old_width, old_height=img.size\n",
        "    block_num=max(int(max(old_width, old_height)/16),block_num_min)\n",
        "    img_reference=to_block(img,block_num,2**block_level)\n",
        "\n",
        "    b_image=block_level*block_num**2\n",
        "elif mode=='net' or mode =='tuned_net':\n",
        "    img, old_width, old_height = image_paddle_in(img, 64)\n",
        "    x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out_net = comp_net.forward(x)\n",
        "    out_net['x_hat'].clamp_(0, 1)\n",
        "    img_reference = transforms.ToPILImage()(out_net['x_hat'].squeeze().cpu())\n",
        "    img_reference = image_paddle_out(img_reference, old_width, old_height)\n",
        "    b_image=compute_bpp(out_net)*img.size[0]*img.size[1]\n",
        "elif mode=='ref':\n",
        "    old_width, old_height=img.size\n",
        "    img_reference = Image.open(ref_path).convert('RGB')\n",
        "    b_image=ref_bpp*img.size[0]*img.size[1]\n",
        "\n",
        "\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "GRUiSYokEoT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.image import auto_resize, pad\n",
        "import einops\n",
        "from utils.common import instantiate_from_config, load_state_dict\n",
        "from omegaconf import OmegaConf\n",
        "from model.cldm import ControlLDM\n",
        "from model.spaced_sampler import SpacedSampler\n",
        "model: ControlLDM = instantiate_from_config(OmegaConf.load('./configs/model/cldm.yaml'))\n",
        "ckpt_swinir='./weights/general_full_v1.ckpt'\n",
        "#ckpt_net='./weights/cheng_small.pth.tar'\n"
      ],
      "metadata": {
        "id": "Bf7ofKGFK9ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_state_dict(model, torch.load(ckpt_swinir, map_location=\"cpu\"), strict=True)\n",
        "model.freeze()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "05800gYuPqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Decoder ##########################\n",
        "num_inference_steps=40\n",
        "exag=1024/max(img_reference.size)\n",
        "height=int(img_reference.size[1]*exag/8)*8\n",
        "width=int(img_reference.size[0]*exag/8)*8\n",
        "\n",
        "#    img_reference=img_reference.resize([width,height])\n",
        "mask_0=mask_0.resize([width,height])\n",
        "mask_1=mask_1.resize([width,height])\n",
        "mask_2=mask_2.resize([width,height])\n",
        "mask_all=Image.new(\"RGB\", img_reference.size, (255, 255, 255))\n",
        "\n",
        "image = img_reference\n",
        "\n",
        "if using_map:\n",
        "    b_mask=mask_num*mask_num*3\n",
        "    b_word=(len(detail_0)+len(detail_1)+len(detail_2)+len(detail_all))*8\n",
        "    bpp=(b_image+b_mask+b_word)/(img.size[0]*img.size[1])\n",
        "    print('bpp='+str(bpp))\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_0,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_0.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_0)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask0/'+image_name)\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_1,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_1.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_1)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask1/'+image_name)\n",
        "\n",
        "    image_tmp = sr_pipe(image,positive_prompt=detail_2,cfg=3.5,steps=3,res=res)\n",
        "    image = ImageChops.add(ImageChops.multiply(image_tmp,mask_2.convert(\"RGB\")),\n",
        "                          ImageChops.multiply(image,Image.fromarray(255-np.array(mask_2)).convert(\"RGB\"))\n",
        "                          ).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'Mask2/'+image_name)\n",
        "\n",
        "    image = sr_pipe(image,positive_prompt=detail_all,cfg=7,steps=40,res=res).resize((old_width, old_height))\n",
        "#    image.resize((old_width, old_height)).save(output_folder+'SR/'+image_name)\n",
        "\n",
        "else:\n",
        "    b_word=(len(detail_all))*8\n",
        "    bpp=(b_image+b_word)/(img.size[0]*img.size[1])\n",
        "    print('image bit = ', b_image, 'text bit = ', b_word, 'bpp=', str(bpp))\n",
        "\n",
        "    image = sr_pipe(image,positive_prompt=detail_all,cfg=7,steps=40,res=res,\n",
        "                    old_size = (old_width, old_height))\n",
        "    #output_image = image.resize((old_width, old_height))"
      ],
      "metadata": {
        "id": "cCVF3b3eI2lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Image (Compressed & Decompressed & Deblurred)"
      ],
      "metadata": {
        "id": "uNEusZU0eOfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image[0]"
      ],
      "metadata": {
        "id": "GDS0siKav1CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Image"
      ],
      "metadata": {
        "id": "MsyQGpCLefx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "94ZpiVgvWNyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "from lpips import LPIPS\n",
        "\n",
        "\n",
        "def psnr(img0, img1):\n",
        "    mse = np.mean((img0 - img1) ** 2)\n",
        "    return 10 * np.log10(255 ** 2 / mse)\n",
        "\n",
        "def lpips(img0, img1):\n",
        "    # Variables im0, im1 is a PyTorch Tensor/Variable with shape Nx3xHxW\n",
        "    # (N patches of size HxW, RGB images scaled in [-1,+1])\n",
        "    loss_fn_vgg = LPIPS(net='vgg')\n",
        "    img0 = (TF.to_tensor(img0) - 0.5) * 2\n",
        "    img0.unsqueeze(0)\n",
        "\n",
        "    img1 = (TF.to_tensor(img1) - 0.5) * 2\n",
        "    img1.unsqueeze(0)\n",
        "    # Higher means further/more different. Lower means more similar.\n",
        "    return loss_fn_vgg(img0, img1).item()\n",
        "\n"
      ],
      "metadata": {
        "id": "VByb43NHxnFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.array(img, dtype = np.float32)\n",
        "image[0] = np.array(image[0], dtype = np.float32)"
      ],
      "metadata": {
        "id": "kca2QZKG9vmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpips: \", lpips(img, image[0]))\n",
        "print(\"psnr: \", psnr(img, image[0]))"
      ],
      "metadata": {
        "id": "nQiJOLyhUQKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhfY-AOc_qB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}